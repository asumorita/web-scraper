import streamlit as st
import requests
from bs4 import BeautifulSoup
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ASU Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ•·ï¸",
    layout="centered"
)

# ãƒ˜ãƒƒãƒ€ãƒ¼
st.markdown("### ğŸ¢ ASU")
st.title("ğŸ•·ï¸ Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
st.write("Webãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨H1è¦‹å‡ºã—ã‚’å–å¾—ã—ã¾ã™")

st.markdown("---")

# èª¬æ˜
with st.expander("ğŸ“– ä½¿ã„æ–¹"):
    st.markdown("""
    ### ã“ã®ãƒ„ãƒ¼ãƒ«ã§ã§ãã‚‹ã“ã¨
    - Webãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    - H1è¦‹å‡ºã—ã‚’å–å¾—
    - ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æƒ…å ±ã‚’ç¢ºèª
    
    ### ä½¿ã„æ–¹
    1. URLã‚’å…¥åŠ›
    2. ã€Œæƒ…å ±ã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
    3. ãƒšãƒ¼ã‚¸æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    
    ### æ³¨æ„
    - ä¸€éƒ¨ã®ã‚µã‚¤ãƒˆã¯ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ãŒã‚ã‚Šã¾ã™
    - å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯å„ã‚µã‚¤ãƒˆã®è¦ç´„ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    """)

st.markdown("---")

# URLå…¥åŠ›
st.subheader("ğŸ”— URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

url = st.text_input(
    "Webãƒšãƒ¼ã‚¸ã®URL",
    placeholder="https://example.com",
    help="httpsã‹ã‚‰å§‹ã¾ã‚‹å®Œå…¨ãªURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
)

# ã‚µãƒ³ãƒ—ãƒ«URL
st.markdown("**ã‚µãƒ³ãƒ—ãƒ«URLï¼ˆã‚¯ãƒªãƒƒã‚¯ã§ã‚³ãƒ”ãƒ¼ï¼‰:**")
sample_urls = [
    "https://www.yahoo.co.jp/",
    "https://news.yahoo.co.jp/",
    "https://www.nhk.or.jp/",
]

for sample_url in sample_urls:
    st.code(sample_url)

# å–å¾—ãƒœã‚¿ãƒ³
if st.button("ğŸ•·ï¸ æƒ…å ±ã‚’å–å¾—", type="primary", use_container_width=True):
    
    if not url:
        st.error("âŒ URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    elif not url.startswith(("http://", "https://")):
        st.error("âŒ URLã¯ http:// ã¾ãŸã¯ https:// ã‹ã‚‰å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
    else:
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®ãµã‚Šã‚’ã™ã‚‹ï¼‰
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            with st.spinner("ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­..."):
                # ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                
                # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤å®š
                response.encoding = response.apparent_encoding
            
            # HTMLã‚’è§£æ
            soup = BeautifulSoup(response.content, 'html.parser')
            
            st.success("âœ… ãƒšãƒ¼ã‚¸ã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸï¼")
            
            st.markdown("---")
            st.subheader("ğŸ“Š å–å¾—ã—ãŸæƒ…å ±")
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            if title:
                st.markdown("### ğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«")
                st.info(title.get_text(strip=True))
            else:
                st.warning("âš ï¸ ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            st.markdown("---")
            
            # H1è¦‹å‡ºã—ã‚’å–å¾—
            h1_tags = soup.find_all('h1')
            if h1_tags:
                st.markdown("### ğŸ“Œ H1è¦‹å‡ºã—")
                for idx, h1 in enumerate(h1_tags, 1):
                    h1_text = h1.get_text(strip=True)
                    if h1_text:
                        st.write(f"{idx}. {h1_text}")
            else:
                st.info("â„¹ï¸ H1è¦‹å‡ºã—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            st.markdown("---")
            
            # H2è¦‹å‡ºã—ã‚’å–å¾—
            h2_tags = soup.find_all('h2', limit=5)  # æœ€åˆã®5å€‹ã ã‘
            if h2_tags:
                st.markdown("### ğŸ“ H2è¦‹å‡ºã—ï¼ˆæœ€å¤§5å€‹ï¼‰")
                for idx, h2 in enumerate(h2_tags, 1):
                    h2_text = h2.get_text(strip=True)
                    if h2_text:
                        st.write(f"{idx}. {h2_text}")
            
            st.markdown("---")
            
            # ãƒªãƒ³ã‚¯æ•°ã‚’å–å¾—
            links = soup.find_all('a')
            st.markdown("### ğŸ”— ãƒšãƒ¼ã‚¸æƒ…å ±")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ãƒªãƒ³ã‚¯æ•°", len(links))
            
            with col2:
                images = soup.find_all('img')
                st.metric("ç”»åƒæ•°", len(images))
            
            with col3:
                paragraphs = soup.find_all('p')
                st.metric("æ®µè½æ•°", len(paragraphs))
            
            st.markdown("---")
            
            # ãƒ¡ã‚¿æƒ…å ±
            st.markdown("### ğŸ” ãƒ¡ã‚¿æƒ…å ±")
            
            # description
            description = soup.find('meta', attrs={'name': 'description'})
            if description and description.get('content'):
                st.write("**èª¬æ˜æ–‡:**")
                st.info(description.get('content'))
            
            # keywords
            keywords = soup.find('meta', attrs={'name': 'keywords'})
            if keywords and keywords.get('content'):
                st.write("**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**")
                st.info(keywords.get('content'))
            
            st.markdown("---")
            
            # HTMLã‚½ãƒ¼ã‚¹ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            with st.expander("ğŸ”§ HTMLã‚½ãƒ¼ã‚¹ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰"):
                html_text = str(soup)[:1000]
                st.code(html_text, language='html')
        
        except requests.exceptions.Timeout:
            st.error("âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™")
        
        except requests.exceptions.ConnectionError:
            st.error("âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        except requests.exceptions.HTTPError as e:
            st.error(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {e}")
            st.info("â„¹ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã¯ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# æ³¨æ„äº‹é …
st.markdown("---")
st.warning("""
âš ï¸ **æ³¨æ„äº‹é …**
- Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯å„ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã¾ã™
- å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ç‰¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™
- ã“ã®ãƒ„ãƒ¼ãƒ«ã¯å­¦ç¿’ç›®çš„ã§ä½œæˆã•ã‚Œã¦ã„ã¾ã™
""")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.caption("ğŸ•·ï¸ ASU - Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
st.caption("Created with â¤ï¸ by ASU")
